table(bf2$sal_int)
baseline_rows <- long_df_agg[grepl("baseline", scenario)]
full_rows     <- long_df_agg[grepl("full", scenario)]
# Helper table for join
sDT <- data.table(strategy = unique_strategies, dummy = 1L)
# Add dummy column to template rows
baseline_rows[, dummy := 1L]
full_rows[, dummy := 1L]
# Cartesian merge separately
base_expanded <- sDT[baseline_rows, on = "dummy", allow.cartesian = TRUE][
, dummy := NULL][]
full_expanded <- sDT[full_rows, on = "dummy", allow.cartesian = TRUE][
, dummy := NULL][]
rm(list=ls())
setwd("C:/Users/DaSchulz/OneDrive - European Forest Institute/Dokumente/research/bcnfire")
source("./src/bcn_funs.R")
# libraries
library(tidyverse)
library(future)
library(future.apply)
library(progressr)
library(data.table)
library(data.table)
# ============================================================
# 0. Load & prepare
# ============================================================
long_df_agg <- readRDS("./data/rdat/mc_res_agg.rds")
setDT(long_df_agg)
# ============================================================
# 1. Duplicate baseline/full rows for each non-(baseline|full) strategy
# ============================================================
unique_strategies <- unique(long_df_agg[!grepl("baseline|full", strategy), strategy])
baseline_rows <- long_df_agg[grepl("baseline", scenario)]
full_rows     <- long_df_agg[grepl("full", scenario)]
# Helper table for join
sDT <- data.table(strategy = unique_strategies, dummy = 1L)
# Add dummy column to template rows
baseline_rows[, dummy := 1L]
full_rows[, dummy := 1L]
# Cartesian merge separately
base_expanded <- sDT[baseline_rows, on = "dummy", allow.cartesian = TRUE][
, dummy := NULL][]
full_expanded <- sDT[full_rows, on = "dummy", allow.cartesian = TRUE][
, dummy := NULL][]
# Combine and append
rep_rows <- rbindlist(list(base_expanded, full_expanded), use.names = TRUE, fill = TRUE)
# Append and drop original baseline/full
long_df_agg <- rbindlist(list(long_df_agg, rep_rows), use.names = TRUE, fill = TRUE)
cat("After append:", nrow(long_df_agg), "\n")
print(table(long_df_agg$sal_int))
table(long_df_agg$strategy)
table(long_df_agg$strategy, long_df_agg$sal_int)
long_df_agg <- long_df_agg[!grepl("baseline|full", strategy)]
table(long_df_agg$strategy, long_df_agg$sal_int)
names(long_df_agg)
names(long_df_agg$scenario)
names(long_df_agg$strategy)
table(long_df_agg$strategy)
table(long_df_agg$scenario)
# ============================================================
# 0. Load & prepare
# ============================================================
long_df_agg <- readRDS("./data/rdat/mc_res_agg.rds")
setDT(long_df_agg)
# Helper table for join
sDT <- data.table(strategy = unique_strategies, dummy = 1L)
# Add dummy column to template rows
baseline_rows[, dummy := 1L]
# Cartesian merge separately
base_expanded <- sDT[baseline_rows, on = "dummy", allow.cartesian = TRUE][
, dummy := NULL][]
table(long_df_agg$sal_int)
# Identify the unique key columns in your data
# Adjust if needed — e.g. replace "ign_id" with "fire_id" or similar
key_cols <- intersect(
c("year", "time", "strategy", "scenario", "sal_int", "ignition_id"),
names(long_df_agg)
)
key_cols
# Count duplicates for sal_int == 100
dups <- long_df_agg[sal_int == 100, .N, by = key_cols][N > 1]
cat("Duplicate rows at sal_int = 100:", nrow(dups), "\n")
# Identify the unique key columns in your data
# Adjust if needed — e.g. replace "ign_id" with "fire_id" or similar
key_cols <- intersect(
c("year", "time", "strategy", "sal_int", "ignition_id"),
names(long_df_agg)
)
# Count duplicates for sal_int == 100
dups <- long_df_agg[sal_int == 100, .N, by = key_cols][N > 1]
cat("Duplicate rows at sal_int = 100:", nrow(dups), "\n")
# ----------------------------------
# Final combine
# ----------------------------------
agg_files <- list.files(out_dir, pattern = "batch_lc_agg_.*\\.rds", full.names = TRUE)
# ----------------------------------
# Prepare batching + output
# ----------------------------------
out_dir <- "./data/rdat/mc_fire_batches/"
# ----------------------------------
# Final combine
# ----------------------------------
agg_files <- list.files(out_dir, pattern = "batch_lc_agg_.*\\.rds", full.names = TRUE)
t1 <- readRDS(agg_files[[1]])
table(t1$time)
table(t1$scenario)
table(table(t1$scenario))
table(table(t1$ignition_id))
t2 <- t1 %>%
mutate(
# year = last four characters of scenario
year = substr(scenario, nchar(scenario) - 3, nchar(scenario)),
# logging intensity 0 for baseline, 100 for full
# pattern between _t and _y
sal_int = case_when(grepl("baseline", scenario) ~ "0",
grepl("full", scenario) ~ "100",
TRUE ~ sub(".*_t(\\d+).*", "\\1", scenario)),
# strategy is string before _t pattern starts
strategy = sub("_(t|y).*", "", scenario),
strategy = case_when(
grepl("bau_ROS", strategy) ~ "High Biomass",
grepl("bmr_FireConn", strategy) ~ "High Connectivity",
grepl("random", strategy) ~ "Random",
grepl("sal_costs_cell", strategy) ~ "Low SAL cost",
grepl("lc_artificial", strategy) ~ "High Built-up",
TRUE ~ strategy
),
# year and sal_int as integers
year = as.integer(year),
sal_int = as.integer(sal_int),
time = as.integer(time) # Ensure time is an integer
)
table(t2$time)
table(table(t2$time))
table(t2$strategy)
table(t2$year)
table(table(t2$ignition_id))
table(t2$sal_int)
t2 <- t1 %>%
filter(!strategy == "full")
t3 <- t2 %>%
filter(!strategy == "full")
table(t3$sal_int)
# load data
grd <- st_read("./data/rdat/grd_filt.gpkg")
names(grd)
set.seed(123)
grd <- grd %>%
mutate(
inaccessibility = scale(max.slope, center = F) + scale(road_distance, center = F),
cost_scaler = inaccessibility / max(inaccessibility),
sal_costs_ha = 1750 + (400 * cost_scaler),
sal_costs_cell = sal_costs_ha  * lc_wildland * 4,
random_target = runif(nrow(grd), 0, 1)
)
hist(grd$random_target)
hist(grd$cost_scaler)
hist(grd$sal_costs_cell)
rm(list=ls())
setwd("C:/Users/DaSchulz/OneDrive - European Forest Institute/Dokumente/research/bcnfire")
source("./src/bcn_funs.R")
# libraries
library(tidyverse)
library(sf)
library(terra)
# load data
grd <- st_read("./data/rdat/grd_filt.gpkg")
set.seed(123)
grd <- grd %>%
mutate(
inaccessibility = scale(max.slope, center = F) + scale(road_distance, center = F),
cost_scaler = inaccessibility / max(inaccessibility),
sal_costs_ha = 1750 + (400 * cost_scaler),
sal_costs_cell = sal_costs_ha  * lc_wildland * 4,
random_target = runif(nrow(grd), 0, 1)
)
# iterate over thresholds in 10% increase
years <- 2040:2050
thresholds <- seq(0.1, 1, by = 0.1)
strategies <- c("random", "sal_costs_cell", "lc_artificial", "biomass", "connectivity")
order_decreasing <- c(FALSE, FALSE, TRUE, TRUE, TRUE)
target_df <- grd %>% st_set_geometry(NULL)
strategies <- c("random_target", "sal_costs_cell", "lc_artificial", "biomass", "connectivity")
# run targeting
for (y in years) {
cat(paste0("Year: ", y, "\n"))
for (s in 1:length(strategies)) {
# optional: target by temporally explicit ROS/CON
if(strategies[s] == "biomass"){
strtgy <- paste0("bau_ROS_", y)
} else if(strategies[s] == "connectivity"){
strtgy <- paste0("bmr_FireConn", y,  "b_raster")
} else {
strtgy <- strategies[s]
}
cat(paste0("- Target: ", strtgy, " (decr.:", order_decreasing[s], ") \n"))
for (i in thresholds) {
#cat(paste0("Threshold: ", i))
target_df <- BCN_target_df(
data = target_df,
original_col = c( paste0("bau_ROS_", y), paste0("bmr_FireConn", y, "b_raster") ),
mask_col = paste0("Logging_", y),
replacement_col = c( paste0("sal_ROS_", y), paste0("bmr_FireConn", y, "s_raster") ),
target_col = strtgy, # Optional: if not provided, random assignment is used
threshold = i,
budget = NULL,
relative = TRUE,
year = y,
target_decreasing = order_decreasing[s],
verbose = F
)
}
}
}
saveRDS(target_df, file = "./data/rdat/target_df.rds")
names(grd)[1:11]
names(grd)
rm_cols <- names(grd)[!names(grd) %in% c(names(grd)[1:11], "sal_costs_cell", "geom")]
# --- Reshape the dataframe from wide to long format ---
target_df_long = target_df %>%
select(-all_of(rm_cols)) %>%
#st_set_geometry(NULL) %>%
pivot_longer(
cols = matches("^(new_mask|ros_trg|con_trg)_[a-zA-Z0-9_]+_t\\d+_y\\d+$"), # Select columns matching the new pattern
names_to = c(".value", "strategy", "threshold", "year"), # Use .value, and new 'strategy' and 'threshold' columns
names_pattern = "^(new_mask|ros_trg|con_trg)_([a-zA-Z0-9_]+)_t(\\d+)_y(\\d+)$" # Regex to capture type, strategy, and threshold
) %>%
mutate(
threshold = as.numeric(threshold), # Convert the 'threshold' column to numeric
sal_costs_cell = new_mask * sal_costs_cell
)
# aggregate by threshold (mean and sum)
target_df_agg <- target_df_long %>%
#st_set_geometry(NULL) %>%
group_by(strategy, threshold, year) %>%
summarise(
sum_SAL_ha = sum(new_mask * lc_wildland * 4, na.rm = TRUE),
sum_SAL_costs = sum(sal_costs_cell * new_mask, na.rm = TRUE)
) |>
# rename strategies to more readable
mutate(strategy = case_when(
strategy == "random" ~ "Random",
strategy == "sal_costs_cell"  ~ "Low logging costs",
strategy == "road_distance" ~ "Lowest road distance",
strategy == "lc_artificial" ~ "Highest artificial",
grepl("biomass|bau_ROS", strategy)  ~ "Highest biomass",
grepl("connectivity|FireConn", strategy)  ~ "Highest connectivity",
TRUE ~ strategy
))
# maximum sum_SAL_costs per year
target_df_agg$max_cost <- ave(target_df_agg$sum_SAL_costs, target_df_agg$year, FUN = max)
target_df_agg$max_cost_scaled <- round(target_df_agg$max_cost / max(target_df_agg$max_cost), 4)
target_df_agg$max_sal_area_km2 <- ave(target_df_agg$sum_SAL_ha, target_df_agg$year, FUN = max) / 100 # convert ha to km2
target_df_agg$facet_lab <- paste0(target_df_agg$year, " (max. area: ", round(target_df_agg$max_sal_area_km2, 2), " km²)")
# save data
saveRDS(target_df_agg, file = "./data/rdat/target_df_agg.rds")
# aggregate years (only sum_SAL_ha and sum_SAL_costs)
target_df_agg_annual <- target_df_agg %>%
filter(threshold==100, strategy == "Random")
table(target_df_agg$strategy)
# aggregate by threshold (mean and sum)
target_df_agg <- target_df_long %>%
#st_set_geometry(NULL) %>%
group_by(strategy, threshold, year) %>%
summarise(
sum_SAL_ha = sum(new_mask * lc_wildland * 4, na.rm = TRUE),
sum_SAL_costs = sum(sal_costs_cell * new_mask, na.rm = TRUE)
) |>
# rename strategies to more readable
mutate(strategy = case_when(
strategy == "random_target" ~ "Random",
strategy == "sal_costs_cell"  ~ "Low logging costs",
strategy == "road_distance" ~ "Lowest road distance",
strategy == "lc_artificial" ~ "Highest artificial",
grepl("biomass|bau_ROS", strategy)  ~ "Highest biomass",
grepl("connectivity|FireConn", strategy)  ~ "Highest connectivity",
TRUE ~ strategy
))
# maximum sum_SAL_costs per year
target_df_agg$max_cost <- ave(target_df_agg$sum_SAL_costs, target_df_agg$year, FUN = max)
target_df_agg$max_cost_scaled <- round(target_df_agg$max_cost / max(target_df_agg$max_cost), 4)
target_df_agg$max_sal_area_km2 <- ave(target_df_agg$sum_SAL_ha, target_df_agg$year, FUN = max) / 100 # convert ha to km2
target_df_agg$facet_lab <- paste0(target_df_agg$year, " (max. area: ", round(target_df_agg$max_sal_area_km2, 2), " km²)")
# save data
saveRDS(target_df_agg, file = "./data/rdat/target_df_agg.rds")
# aggregate years (only sum_SAL_ha and sum_SAL_costs)
target_df_agg_annual <- target_df_agg %>%
filter(threshold==100, strategy == "Random")
# barplot total area per year
p.sal.area <- ggplot(target_df_agg_annual, aes(x = factor(year), y = sum_SAL_ha)) +
geom_bar(stat = "identity", position = "dodge") +
# format Y to show values in km2
scale_y_continuous(labels = scales::label_number(scale = 1e-3, suffix = "k"),
expand = expansion(mult = c(0, 0.2))) + # Convert ha to km2
# add costs as label above each bar
geom_text(aes(label = scales::label_number(scale = 1e-6, suffix = "M")(sum_SAL_costs)),
vjust = -0.5, size = 3.5) +
labs(
x = "Share of cells salvage-logged (%)",
y = "Targeted area (ha)"
) +
theme_minimal()
p.sal.area
ggsave(p.sal.area, filename = "./out/fig/sal_area_per_year.png",
width = 6, height = 3, dpi = 300, bg = "white")
# alternative: express in relative terms cmopared to random scenario
target_df_rdm <- target_df_agg %>%
filter(strategy == "Random") %>%
# drop groups
ungroup() %>%
select(year, threshold, sum_SAL_costs) %>%
rename(rdm_sum_SAL_costs = sum_SAL_costs)
target_df_rel <- target_df_agg %>%
filter(strategy != "Random") %>%
left_join(target_df_rdm, by = c("year", "threshold")) %>%
mutate(
rel_sum_SAL_costs = (sum_SAL_costs / rdm_sum_SAL_costs) - 1
) %>%
select(-rdm_sum_SAL_costs)
# 1. Define the color palette and labels for the 'strategy' variable
strategy_colors <- c(
"Highest biomass"  = "darkgreen",
"Highest connectivity" = "purple",
"Highest artificial" = "red",
"Random" = "orange",
"Low logging costs" = "blue"
)
# make area plot
p.rel.smooth <- ggplot(target_df_rel, aes(x = threshold, y = rel_sum_SAL_costs*100, color = strategy)) +
geom_point(alpha = 0.2) +
# smooth line aggregating years
geom_smooth(method = "loess", se = TRUE, lwd = 1.2) +
# add horizontal line at y = 0
geom_hline(yintercept = 0, linetype = "dashed", color = "grey50") +
labs(
x = "Share of cells salvage-logged (%)",
y = "Relative costs (%)"
) +
theme_minimal() +
# multiply y-axis by 100 to express in percentage
scale_y_continuous(labels = scales::label_percent(scale = 1, accuracy = 1),
breaks = seq(-70, 10, by = 10),
expand = expansion(mult = c(0, 0.1))) + # Expand y-axis to avoid clipping
scale_x_continuous(breaks = seq(10, 100, by = 10)) +
scale_color_manual(values = strategy_colors) #+
p.rel.smooth
# save smooth plot
ggsave(p.rel.smooth, filename = "./out/fig/cum_sal_costs_relative_smooth.png",
width = 6, height = 4, dpi = 300, bg = "white")
# plot absolute area on X, Costs on Y, color strategy, symbol as year and size as threshold
p.sal.area.costs <- ggplot(target_df_agg, aes(x = factor(threshold), y = sum_SAL_costs, color = strategy)) +
geom_boxplot(position = "dodge") +
scale_y_continuous(labels = scales::label_number(scale = 1e-6, suffix = "M"),
transform = "log") + # Convert EUR to million EUR
# horizontal line at y = 1000000
geom_hline(yintercept = 1000000, linetype = "dashed", color = "grey50") +
geom_hline(yintercept = 10000000, linetype = "dashed", color = "grey50") +
# add text labels for hlines
annotate("text", x = factor(70), y = 1500000, label = "1M EUR", color = "grey50", size = 3) +
annotate("text", x = factor(30), y = 15000000, label = "10M EUR", color = "grey50", size = 3) +
labs(
x = "",
y = "Total costs (€)",
shape = "Year",
size = "Threshold (%)"
) +
theme_minimal()+
scale_color_manual(values = strategy_colors)
p.sal.area.costs
ggsave(p.sal.area.costs, filename = "./out/fig/sal_area_costs.png",
width = 6, height = 4, dpi = 300, bg = "white")
# combine plots
library(ggpubr)
p.combined <- ggarrange(
p.sal.area.costs,
p.rel.smooth,
ncol = 1, nrow = 2,
labels = c("A", "B"),
align = "hv",
common.legend = TRUE, legend = "bottom"
)
p.combined
ggsave(p.combined, filename = "./out/fig/sal_area_costs_combined.png",
width = 7, height = 6, dpi = 300, bg = "white")
setwd("C:/Users/DaSchulz/OneDrive - European Forest Institute/Dokumente/research/bcnfire")
# load all shp files in subfolder of ./data/wildfires/wildfires_shp
library(sf)
library(tidyverse)
library(stringr)
library(osmextract)
library(fs)
# study region: Catalunya
# load grd shape
grd <- st_read("./data/rdat/grd_filt.gpkg")
comarcas <- st_read("./data/divisions-administratives-v2r1-20250101/divisions-administratives-v2r1-comarques-1000000-20250101.json") |>
st_transform(crs = st_crs(grd)) |>
st_sf() |>
st_make_valid()
aoi <- st_union(comarcas)
# make 1km grid across study region
catalunya_grd <- st_make_grid(comarcas,
cellsize = units::as_units(1000, "meter")) |>
st_make_valid() |>
st_intersection(aoi)
# load and extract landcover and topography
path_topo <- "./data/topo/topo_bcn.tif"
path_lc_rcl <- "./data/landcover/BMRlandcover_rcl.tif"
path_infra <- "./data/rdat/r_infrastructure.tif"
library(terra)
rast_topo <- rast(path_topo)
rast_lc <- rast(path_lc_rcl)
rast_infra <- raster::raster(path_infra)
rast_topo <- raster::raster(path_topo)
rast_lc <- raster::raster(path_lc_rcl)
library(dismo)
library(terra)
library(sf)
library(dplyr)
# load wildfire data, calculate centroids,
wildfires <- list.files("./data/wildfires/wildfires_shp", pattern = ".shp$",
recursive = TRUE, full.names = TRUE, include.dirs = TRUE) |>
lapply(st_read) |>
bind_rows()
wildfires <- wildfires |>
filter(!is.na(CODI_FINAL)) |>
mutate(
year = case_when(nchar(DATA_INCEN) == 7 ~ as.integer(paste0("20", substr(DATA_INCEN, 6, 7))),
nchar(DATA_INCEN) == 8 ~ as.integer(paste0("20", substr(DATA_INCEN, 7, 8))),
nchar(DATA_INCEN) >= 10 ~ as.integer(substr(DATA_INCEN, 7, 10)),
TRUE ~ NA),
month = case_when(nchar(DATA_INCEN) == 7 ~ as.integer(substr(DATA_INCEN, 3, 4)),
nchar(DATA_INCEN) >= 8 ~ as.integer(substr(DATA_INCEN, 4, 5)),
TRUE ~ NA),
# day is anything before first slash
day = as.integer(substring(DATA_INCEN, 1, regexpr("/", DATA_INCEN) - 1)),
date = as.Date(paste(year, month, day, sep = "-"), format = "%Y-%m-%d"),
area_ha = as.numeric(st_area(geometry)) / 10000,  # convert from m² to ha
MUNICIPI = str_to_title(MUNICIPI)  # capitalize first letter of each word
) |>
select(CODI_FINAL, MUNICIPI, date, year, month, area_ha, geometry)
# transform crs
wildfires <- st_transform(wildfires, crs = st_crs(grd)) %>%
st_make_valid()
wildfire_centroids <- st_centroid(wildfires)
mapview::mapview(wildfire_centroids)
# Fire ignition points (sf)
wildfire_points <- wildfire_centroids %>%
st_transform(4326)
# Convert to SpatialPoints for dismo
wildfire_sp <- as_Spatial(st_geometry(wildfire_points))
env_stack <- stack(rast_topo, rast_lc, rast_infra)
path_topo <- "./data/rdat/r_topo_catalunya.tif"
path_lc_rcl <- "./data/rdat/r_lc_catalunya.tif"
rast_infra <- raster::raster(path_infra)
rast_topo <- raster::raster(path_topo)
rast_lc <- raster::raster(path_lc_rcl)
env_stack <- stack(rast_topo, rast_lc, rast_infra)
# Convert SpatRaster to RasterStack
library(raster)
# MaxEnt requires Java — install Java JDK if not present
mask <- vect(aoi)
# Run MaxEnt
me <- dismo::maxent(
x = env_stack,
p = wildfire_sp,
path = "./data/models/maxent_fire/",
args = c("replicates=1", "responsecurves=true")
)
# check NA
na_ex <- extract(env_stack, wildfire_sp)
View(na_ex)
table(is.na(na_ex))
complete.cases(na_ex)
wildfire_sp <- wildfire_sp[complete.cases(na_ex),]
# Run MaxEnt
me <- dismo::maxent(
x = env_stack,
p = wildfire_sp,
path = "./data/models/maxent_fire/",
args = c("replicates=1", "responsecurves=true")
)
# Run MaxEnt
me <- dismo::maxent(
x = env_stack,
p = wildfire_sp,
path = "./data/maxent_fire/",
args = c("replicates=1", "responsecurves=true")
)
head(wildfire_sp)
head(wildfires)
170/60000
500/60000
rm(list=ls())
# working directory
setwd("C:/Users/DaSchulz/OneDrive - European Forest Institute/Dokumente/research/bcnfire")
# libraries
library(terra)
library(sf)
library(tidyverse)
library(exactextractr)
library(spdep) # for poly2nb
path_wui <- "./data/wui/WUI_Catalonia.tif"
# 1. Read the spatial data frame
grd <- st_read("./data/rdat/grd_filt.gpkg")
# load wildland urba interface
wui <- rast(path_wui)
wui_e <- exactextractr::exact_extract(wui, grd, fun="frac")
head(wui_e)
names(wui_e) <- c("wui_low", "wui_medium", "wui_nohouse", "wui_verylow", "wui_intermix", "wui_interface")
grd <- bind_cols(grd, wui_e)
names(grd)
write_sf(grd, "./data/rdat/grd_filt.gpkg")
